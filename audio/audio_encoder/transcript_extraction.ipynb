{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Whisper Transcript Preprocessing Pipeline\n",
    "This notebook precomputes Whisper transcripts for all audio samples with three different time windows.\n",
    "The cached transcripts will be used during training to avoid expensive inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import soundfile as sf\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "import pickle\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ Imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9e24d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "config",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "config_cell",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Configuration\u001b[39;00m\n\u001b[1;32m      2\u001b[0m CONFIG \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcsv_path\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../SoccerNet_audio_labels.csv\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput_dir\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../transcript_cache\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwhisper_model\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mopenai/whisper-base\u001b[39m\u001b[38;5;124m'\u001b[39m,  \u001b[38;5;66;03m# or 'small', 'medium', 'large'\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtranslate_to_english\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m,  \u001b[38;5;66;03m# Set to True to match your encoder\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_samples\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m,  \u001b[38;5;66;03m# Set to a number for testing, None for full dataset\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m8\u001b[39m,  \u001b[38;5;66;03m# Process multiple samples at once for speed\u001b[39;00m\n\u001b[1;32m     10\u001b[0m }\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Window configurations: (seconds_before, seconds_after, dataset_name)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m WINDOW_CONFIGS \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     14\u001b[0m     (\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwindow_5s_centered\u001b[39m\u001b[38;5;124m'\u001b[39m),      \u001b[38;5;66;03m# ¬±5 seconds around event\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     (\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwindow_10s_centered\u001b[39m\u001b[38;5;124m'\u001b[39m),   \u001b[38;5;66;03m# ¬±10 seconds around event  \u001b[39;00m\n\u001b[1;32m     16\u001b[0m     (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwindow_10s_after\u001b[39m\u001b[38;5;124m'\u001b[39m)        \u001b[38;5;66;03m# 10 seconds after event only\u001b[39;00m\n\u001b[1;32m     17\u001b[0m ]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "CONFIG = {\n",
    "    'csv_path': '../SoccerNet_audio_labels.csv',\n",
    "    'output_dir': '../transcript_cache',\n",
    "    'whisper_model': 'openai/whisper-base',  # or 'small', 'medium', 'large'\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    'translate_to_english': True,  # Set to True to match your encoder\n",
    "    'max_samples': None,  # Set to a number for testing, None for full dataset\n",
    "    'batch_size': 8,  # Process multiple samples at once for speed\n",
    "}\n",
    "\n",
    "# Window configurations: (seconds_before, seconds_after, dataset_name)\n",
    "WINDOW_CONFIGS = [\n",
    "    (5, 5, 'window_5s_centered'),      # ¬±5 seconds around event\n",
    "    (10, 10, 'window_10s_centered'),   # ¬±10 seconds around event  \n",
    "    (0, 10, 'window_10s_after')        # 10 seconds after event only\n",
    "]\n",
    "\n",
    "print(f\"üìç Device: {CONFIG['device']}\")\n",
    "print(f\"üé§ Whisper Model: {CONFIG['whisper_model']}\")\n",
    "print(f\"üåç Translate to English: {CONFIG['translate_to_english']}\")\n",
    "print(f\"\\nüìä Window Configurations:\")\n",
    "for before, after, name in WINDOW_CONFIGS:\n",
    "    print(f\"  ‚Ä¢ {name}: {before}s before + {after}s after = {before+after}s total\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model_load",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Load Whisper Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_model",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîß Loading Whisper model...\")\n",
    "\n",
    "processor = WhisperProcessor.from_pretrained(CONFIG['whisper_model'])\n",
    "model = WhisperForConditionalGeneration.from_pretrained(CONFIG['whisper_model'])\n",
    "model = model.to(CONFIG['device'])\n",
    "model.eval()\n",
    "\n",
    "print(f\"‚úÖ Whisper model loaded on {CONFIG['device']}\")\n",
    "print(f\"üì¶ Model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "helper_functions",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio_segment(audio_path, center_time, before_sec, after_sec):\n",
    "    \"\"\"\n",
    "    Load audio segment around an event timestamp.\n",
    "    \n",
    "    Returns:\n",
    "        waveform, sr, actual_start, actual_end\n",
    "    \"\"\"\n",
    "    data, sr = sf.read(audio_path, always_2d=True)\n",
    "    \n",
    "    # Calculate segment boundaries\n",
    "    start_time = max(0, center_time - before_sec)\n",
    "    end_time = center_time + after_sec\n",
    "    \n",
    "    start_frame = int(start_time * sr)\n",
    "    end_frame = int(end_time * sr)\n",
    "    end_frame = min(end_frame, len(data))\n",
    "    \n",
    "    # Extract mono channel\n",
    "    segment = data[start_frame:end_frame, 0]\n",
    "    \n",
    "    actual_start = start_frame / sr\n",
    "    actual_end = end_frame / sr\n",
    "    \n",
    "    return segment, sr, actual_start, actual_end\n",
    "\n",
    "\n",
    "def transcribe_audio(waveform, sr, processor, model, device, translate=True):\n",
    "    \"\"\"\n",
    "    Generate transcript for audio waveform.\n",
    "    \"\"\"\n",
    "    # Process audio for Whisper (expects 16kHz)\n",
    "    input_features = processor(\n",
    "        waveform, \n",
    "        sampling_rate=sr, \n",
    "        return_tensors=\"pt\"\n",
    "    ).input_features.to(device)\n",
    "    \n",
    "    # Generate transcript\n",
    "    generate_opts = {\"task\": \"translate\"} if translate else {}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        predicted_ids = model.generate(input_features, **generate_opts)\n",
    "        transcript = processor.batch_decode(\n",
    "            predicted_ids, \n",
    "            skip_special_tokens=True\n",
    "        )[0].strip()\n",
    "    \n",
    "    return transcript\n",
    "\n",
    "\n",
    "def create_unique_key(audio_path, timestamp):\n",
    "    \"\"\"\n",
    "    Create unique key for caching: {filename}_{timestamp}\n",
    "    \"\"\"\n",
    "    path_stem = Path(audio_path).stem\n",
    "    return f\"{path_stem}_{timestamp:.2f}\"\n",
    "\n",
    "\n",
    "print(\"‚úÖ Helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load_data",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_csv",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"üìÇ Loading dataset from: {CONFIG['csv_path']}\")\n",
    "\n",
    "df = pd.read_csv(CONFIG['csv_path'])\n",
    "\n",
    "if CONFIG['max_samples']:\n",
    "    df = df.head(CONFIG['max_samples'])\n",
    "    print(f\"‚ö†Ô∏è  Limited to {CONFIG['max_samples']} samples for testing\")\n",
    "\n",
    "print(f\"üìä Total samples: {len(df)}\")\n",
    "print(f\"\\nüìã Dataset preview:\")\n",
    "display(df.head())\n",
    "\n",
    "print(f\"\\nüè∑Ô∏è  Label distribution:\")\n",
    "display(df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "initialize_storage",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Initialize Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "init_storage",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "output_path = Path(CONFIG['output_dir'])\n",
    "output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Initialize storage for each window configuration\n",
    "datasets = {}\n",
    "for before_sec, after_sec, name in WINDOW_CONFIGS:\n",
    "    datasets[name] = {\n",
    "        'transcripts': {},\n",
    "        'metadata': {},\n",
    "        'config': {\n",
    "            'before_sec': before_sec,\n",
    "            'after_sec': after_sec,\n",
    "            'total_duration': before_sec + after_sec,\n",
    "            'whisper_model': CONFIG['whisper_model'],\n",
    "            'translate_to_english': CONFIG['translate_to_english']\n",
    "        }\n",
    "    }\n",
    "\n",
    "print(f\"‚úÖ Initialized {len(datasets)} dataset containers\")\n",
    "print(f\"üìÅ Output directory: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "process",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Process All Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "process_samples",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üöÄ Starting transcript generation...\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "failed_samples = []\n",
    "successful = 0\n",
    "\n",
    "# Progress bar for overall processing\n",
    "pbar = tqdm(df.iterrows(), total=len(df), desc=\"Processing samples\")\n",
    "\n",
    "for idx, row in pbar:\n",
    "    audio_path = row['audio_path']\n",
    "    timestamp = row['time_seconds']\n",
    "    label = row['label']\n",
    "    \n",
    "    # Create unique key\n",
    "    key = create_unique_key(audio_path, timestamp)\n",
    "    \n",
    "    try:\n",
    "        # Process each window configuration\n",
    "        for before_sec, after_sec, name in WINDOW_CONFIGS:\n",
    "            # Load audio segment\n",
    "            waveform, sr, actual_start, actual_end = load_audio_segment(\n",
    "                audio_path, timestamp, before_sec, after_sec\n",
    "            )\n",
    "            \n",
    "            # Generate transcript\n",
    "            transcript = transcribe_audio(\n",
    "                waveform, sr, processor, model, \n",
    "                CONFIG['device'], CONFIG['translate_to_english']\n",
    "            )\n",
    "            \n",
    "            # Store transcript and metadata\n",
    "            datasets[name]['transcripts'][key] = transcript\n",
    "            datasets[name]['metadata'][key] = {\n",
    "                'audio_path': audio_path,\n",
    "                'event_timestamp': timestamp,\n",
    "                'label': label,\n",
    "                'segment_start': actual_start,\n",
    "                'segment_end': actual_end,\n",
    "                'segment_duration': actual_end - actual_start,\n",
    "                'sample_rate': sr,\n",
    "                'transcript_length': len(transcript),\n",
    "                'word_count': len(transcript.split())\n",
    "            }\n",
    "        \n",
    "        successful += 1\n",
    "        pbar.set_postfix({'successful': successful, 'failed': len(failed_samples)})\n",
    "    \n",
    "    except Exception as e:\n",
    "        failed_samples.append({\n",
    "            'idx': idx,\n",
    "            'audio_path': audio_path,\n",
    "            'timestamp': timestamp,\n",
    "            'error': str(e)\n",
    "        })\n",
    "        pbar.set_postfix({'successful': successful, 'failed': len(failed_samples)})\n",
    "        continue\n",
    "\n",
    "pbar.close()\n",
    "\n",
    "print(f\"\\n‚úÖ Processing complete!\")\n",
    "print(f\"   Successful: {successful}/{len(df)}\")\n",
    "print(f\"   Failed: {len(failed_samples)}/{len(df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "save",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save_results",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üíæ Saving transcript datasets...\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "for name, data in datasets.items():\n",
    "    # Save as pickle (most efficient for Python)\n",
    "    pickle_path = output_path / f\"transcripts_{name}.pkl\"\n",
    "    with open(pickle_path, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "    \n",
    "    file_size = pickle_path.stat().st_size / (1024 * 1024)  # MB\n",
    "    print(f\"‚úÖ Saved: {pickle_path.name}\")\n",
    "    print(f\"   Transcripts: {len(data['transcripts'])}\")\n",
    "    print(f\"   File size: {file_size:.2f} MB\\n\")\n",
    "    \n",
    "    # Also save as JSON (human-readable backup)\n",
    "    json_path = output_path / f\"transcripts_{name}.json\"\n",
    "    with open(json_path, 'w') as f:\n",
    "        json.dump({\n",
    "            'config': data['config'],\n",
    "            'transcripts': data['transcripts'],\n",
    "            'metadata': data['metadata']\n",
    "        }, f, indent=2)\n",
    "    print(f\"üìÑ JSON backup: {json_path.name}\\n\")\n",
    "\n",
    "# Save failed samples log\n",
    "if failed_samples:\n",
    "    failed_path = output_path / \"failed_samples.json\"\n",
    "    with open(failed_path, 'w') as f:\n",
    "        json.dump(failed_samples, f, indent=2)\n",
    "    print(f\"‚ö†Ô∏è  Failed samples log: {failed_path.name}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéâ All files saved successfully!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statistics",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ Generate Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compute_stats",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüìä Dataset Statistics:\\n\")\n",
    "\n",
    "for name, data in datasets.items():\n",
    "    transcripts = list(data['transcripts'].values())\n",
    "    metadata = list(data['metadata'].values())\n",
    "    \n",
    "    transcript_lengths = [len(t) for t in transcripts]\n",
    "    word_counts = [len(t.split()) for t in transcripts]\n",
    "    durations = [m['segment_duration'] for m in metadata]\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Dataset: {name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Total Samples: {len(transcripts)}\")\n",
    "    print(f\"Avg Transcript Length: {sum(transcript_lengths)/len(transcript_lengths):.1f} chars\")\n",
    "    print(f\"Avg Word Count: {sum(word_counts)/len(word_counts):.1f} words\")\n",
    "    print(f\"Avg Segment Duration: {sum(durations)/len(durations):.2f}s\")\n",
    "    print(f\"Empty Transcripts: {sum(1 for t in transcripts if not t)}\")\n",
    "    \n",
    "    # Label distribution\n",
    "    label_counts = {}\n",
    "    for m in metadata:\n",
    "        label = m['label']\n",
    "        label_counts[label] = label_counts.get(label, 0) + 1\n",
    "    \n",
    "    print(f\"\\nLabel Distribution:\")\n",
    "    for label, count in sorted(label_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "        pct = count / len(transcripts) * 100\n",
    "        print(f\"  {label}: {count} ({pct:.1f}%)\")\n",
    "    \n",
    "    # Save stats to file\n",
    "    stats_path = output_path / f\"stats_{name}.txt\"\n",
    "    with open(stats_path, 'w') as f:\n",
    "        f.write(f\"Transcript Dataset Statistics: {name}\\n\")\n",
    "        f.write(\"=\"*60 + \"\\n\")\n",
    "        f.write(f\"Total Samples: {len(transcripts)}\\n\")\n",
    "        f.write(f\"Avg Transcript Length: {sum(transcript_lengths)/len(transcript_lengths):.1f} chars\\n\")\n",
    "        f.write(f\"Avg Word Count: {sum(word_counts)/len(word_counts):.1f} words\\n\")\n",
    "        f.write(f\"Avg Segment Duration: {sum(durations)/len(durations):.2f}s\\n\")\n",
    "        f.write(f\"Empty Transcripts: {sum(1 for t in transcripts if not t)}\\n\\n\")\n",
    "        f.write(\"Label Distribution:\\n\")\n",
    "        for label, count in sorted(label_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "            pct = count / len(transcripts) * 100\n",
    "            f.write(f\"  {label}: {count} ({pct:.1f}%)\\n\")\n",
    "\n",
    "print(f\"\\n‚úÖ Statistics saved to individual files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sample_view",
   "metadata": {},
   "source": [
    "## üîü View Sample Transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "view_samples",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüìù Sample Transcripts:\\n\")\n",
    "\n",
    "# Show first 5 transcripts from each dataset\n",
    "for name, data in datasets.items():\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Dataset: {name}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    for i, (key, transcript) in enumerate(list(data['transcripts'].items())[:5]):\n",
    "        meta = data['metadata'][key]\n",
    "        print(f\"Sample {i+1}:\")\n",
    "        print(f\"  Key: {key}\")\n",
    "        print(f\"  Label: {meta['label']}\")\n",
    "        print(f\"  Duration: {meta['segment_duration']:.2f}s\")\n",
    "        print(f\"  Transcript: '{transcript}'\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "usage_guide",
   "metadata": {},
   "source": [
    "## ‚úÖ Usage in Training\n",
    "\n",
    "Now you can use these precomputed transcripts in your training script:\n",
    "\n",
    "```python\n",
    "# Load the transcript cache\n",
    "import pickle\n",
    "with open('transcript_cache/transcripts_window_10s_centered.pkl', 'rb') as f:\n",
    "    transcript_data = pickle.load(f)\n",
    "\n",
    "# In your training loop:\n",
    "def get_transcript(audio_path, timestamp):\n",
    "    from pathlib import Path\n",
    "    key = f\"{Path(audio_path).stem}_{timestamp:.2f}\"\n",
    "    return transcript_data['transcripts'][key]\n",
    "\n",
    "# Use it:\n",
    "transcript = get_transcript('path/to/audio.wav', 123.45)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
