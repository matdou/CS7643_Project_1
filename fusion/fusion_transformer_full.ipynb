{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e6390ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hice1/mdoutre3/CS7643_Project_1\n"
     ]
    }
   ],
   "source": [
    "%cd /home/hice1/mdoutre3/CS7643_Project_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc535a99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from glob import glob\n",
    "import math\n",
    "import json\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Running on: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "95cf244b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"Positional encoding for temporal sequences.\"\"\"\n",
    "    def __init__(self, d_model, max_len=600, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() *\n",
    "                            (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# INDIVIDUAL MODALITY MODELS\n",
    "# =============================================================================\n",
    "class VideoOnlyModel(nn.Module):\n",
    "    \"\"\"Video-only baseline using transformer.\"\"\"\n",
    "    def __init__(self, input_dim=768, d_model=256, nhead=4, num_layers=2,\n",
    "                 num_classes=16, dropout=0.1, max_seq_len=100):\n",
    "        super().__init__()\n",
    "        self.input_proj = nn.Linear(input_dim, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, max_seq_len, dropout=dropout)\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=nhead, dim_feedforward=d_model,\n",
    "            dropout=dropout, batch_first=True\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model // 2, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, video_x, video_mask, text_emb, audio_emb):\n",
    "        x = self.input_proj(video_x)\n",
    "        x = self.pos_encoder(x)\n",
    "        x = self.encoder(x, src_key_padding_mask=~video_mask)\n",
    "        x = x.mean(dim=1)  # Average pooling\n",
    "        return self.classifier(x)\n",
    "\n",
    "\n",
    "class TextOnlyModel(nn.Module):\n",
    "    \"\"\"Text-only baseline.\"\"\"\n",
    "    def __init__(self, input_dim=768, hidden_dim=256, num_classes=16, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.LayerNorm(hidden_dim // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim // 2, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, video_x, video_mask, text_emb, audio_emb):\n",
    "        return self.classifier(text_emb)\n",
    "\n",
    "\n",
    "class AudioOnlyModel(nn.Module):\n",
    "    \"\"\"Audio-only baseline.\"\"\"\n",
    "    def __init__(self, input_dim=1024, hidden_dim=256, num_classes=16, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.LayerNorm(hidden_dim // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim // 2, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, video_x, video_mask, text_emb, audio_emb):\n",
    "        return self.classifier(audio_emb)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# BIMODAL FUSION MODELS\n",
    "# =============================================================================\n",
    "class VideoTextFusion(nn.Module):\n",
    "    \"\"\"Video + Text fusion.\"\"\"\n",
    "    def __init__(self, video_dim=768, text_dim=768, d_model=256, nhead=4,\n",
    "                 num_layers=2, num_classes=16, dropout=0.1, max_seq_len=100):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.video_proj = nn.Sequential(nn.Linear(video_dim, d_model), nn.Dropout(dropout))\n",
    "        self.text_proj = nn.Sequential(nn.Linear(text_dim, d_model), nn.Dropout(dropout))\n",
    "        \n",
    "        self.pos_encoder = PositionalEncoding(d_model, max_seq_len + 1, dropout=dropout)\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=nhead, dim_feedforward=d_model,\n",
    "            dropout=dropout, batch_first=True, norm_first=True\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, video_x, video_mask, text_emb, audio_emb):\n",
    "        B = video_x.size(0)\n",
    "        video_x = self.video_proj(video_x)\n",
    "        text_x = self.text_proj(text_emb).unsqueeze(1)\n",
    "        \n",
    "        x = torch.cat([text_x, video_x], dim=1)\n",
    "        text_mask = torch.ones(B, 1, dtype=torch.bool, device=video_mask.device)\n",
    "        full_mask = torch.cat([text_mask, video_mask], dim=1)\n",
    "        \n",
    "        x = self.pos_encoder(x)\n",
    "        x = self.encoder(x, src_key_padding_mask=~full_mask)\n",
    "        \n",
    "        return self.classifier(x[:, 0])\n",
    "\n",
    "\n",
    "class VideoAudioFusion(nn.Module):\n",
    "    \"\"\"Video + Audio fusion.\"\"\"\n",
    "    def __init__(self, video_dim=768, audio_dim=1024, d_model=256, nhead=4,\n",
    "                 num_layers=2, num_classes=16, dropout=0.1, max_seq_len=100):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.video_proj = nn.Sequential(nn.Linear(video_dim, d_model), nn.Dropout(dropout))\n",
    "        self.audio_proj = nn.Sequential(nn.Linear(audio_dim, d_model), nn.Dropout(dropout))\n",
    "        \n",
    "        self.pos_encoder = PositionalEncoding(d_model, max_seq_len + 1, dropout=dropout)\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=nhead, dim_feedforward=d_model,\n",
    "            dropout=dropout, batch_first=True, norm_first=True\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, video_x, video_mask, text_emb, audio_emb):\n",
    "        B = video_x.size(0)\n",
    "        video_x = self.video_proj(video_x)\n",
    "        audio_x = self.audio_proj(audio_emb).unsqueeze(1)\n",
    "        \n",
    "        x = torch.cat([audio_x, video_x], dim=1)\n",
    "        audio_mask = torch.ones(B, 1, dtype=torch.bool, device=video_mask.device)\n",
    "        full_mask = torch.cat([audio_mask, video_mask], dim=1)\n",
    "        \n",
    "        x = self.pos_encoder(x)\n",
    "        x = self.encoder(x, src_key_padding_mask=~full_mask)\n",
    "        \n",
    "        return self.classifier(x[:, 0])\n",
    "\n",
    "\n",
    "class TextAudioFusion(nn.Module):\n",
    "    \"\"\"Text + Audio fusion.\"\"\"\n",
    "    def __init__(self, text_dim=768, audio_dim=1024, hidden_dim=256,\n",
    "                 num_classes=16, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.text_proj = nn.Linear(text_dim, hidden_dim)\n",
    "        self.audio_proj = nn.Linear(audio_dim, hidden_dim)\n",
    "        \n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.LayerNorm(hidden_dim // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim // 2, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, video_x, video_mask, text_emb, audio_emb):\n",
    "        text_x = self.text_proj(text_emb)\n",
    "        audio_x = self.audio_proj(audio_emb)\n",
    "        x = torch.cat([text_x, audio_x], dim=1)\n",
    "        return self.fusion(x)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# TRIMODAL FUSION MODEL\n",
    "# =============================================================================\n",
    "class TrimodalFusion(nn.Module):\n",
    "    \"\"\"Video + Text + Audio fusion.\"\"\"\n",
    "    def __init__(self, video_dim=768, text_dim=768, audio_dim=1024,\n",
    "                 d_model=256, nhead=4, num_layers=2, num_classes=16,\n",
    "                 dropout=0.1, max_seq_len=100):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.video_proj = nn.Sequential(nn.Linear(video_dim, d_model), nn.Dropout(dropout))\n",
    "        self.text_proj = nn.Sequential(nn.Linear(text_dim, d_model), nn.Dropout(dropout))\n",
    "        self.audio_proj = nn.Sequential(nn.Linear(audio_dim, d_model), nn.Dropout(dropout))\n",
    "        \n",
    "        self.pos_encoder = PositionalEncoding(d_model, max_seq_len + 2, dropout=dropout)\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=nhead, dim_feedforward=d_model,\n",
    "            dropout=dropout, batch_first=True, norm_first=True\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, video_x, video_mask, text_emb, audio_emb):\n",
    "        B = video_x.size(0)\n",
    "        \n",
    "        video_x = self.video_proj(video_x)\n",
    "        text_x = self.text_proj(text_emb).unsqueeze(1)\n",
    "        audio_x = self.audio_proj(audio_emb).unsqueeze(1)\n",
    "        \n",
    "        x = torch.cat([text_x, audio_x, video_x], dim=1)\n",
    "        \n",
    "        special_mask = torch.ones(B, 2, dtype=torch.bool, device=video_mask.device)\n",
    "        full_mask = torch.cat([special_mask, video_mask], dim=1)\n",
    "        \n",
    "        x = self.pos_encoder(x)\n",
    "        x = self.encoder(x, src_key_padding_mask=~full_mask)\n",
    "        \n",
    "        return self.classifier(x[:, 0])\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# DATASET\n",
    "# =============================================================================\n",
    "class SoccerTrimodalDataset(Dataset):\n",
    "    \"\"\"Dataset for video + text + audio fusion.\"\"\"\n",
    "    def __init__(self, video_paths, text_paths, audio_paths, labels,\n",
    "                 max_seq_len=100, augment=False):\n",
    "        self.video_paths = video_paths\n",
    "        self.text_paths = text_paths\n",
    "        self.audio_paths = audio_paths\n",
    "        self.labels = labels\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.augment = augment\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.video_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Load video features (768-dim)\n",
    "        video = np.load(self.video_paths[idx])\n",
    "        video = torch.from_numpy(video).float()\n",
    "        T = video.shape[0]\n",
    "        \n",
    "        # Pad or truncate video\n",
    "        if T > self.max_seq_len:\n",
    "            video = video[:self.max_seq_len]\n",
    "            mask = torch.ones(self.max_seq_len, dtype=torch.bool)\n",
    "        else:\n",
    "            pad = torch.zeros(self.max_seq_len - T, 768)\n",
    "            video = torch.cat([video, pad], dim=0)\n",
    "            mask = torch.cat([\n",
    "                torch.ones(T, dtype=torch.bool),\n",
    "                torch.zeros(self.max_seq_len - T, dtype=torch.bool)\n",
    "            ])\n",
    "        \n",
    "        # Load text embedding (768-dim)\n",
    "        text_data = torch.load(self.text_paths[idx])\n",
    "        text_emb = text_data[\"embedding\"]\n",
    "        if text_emb.dim() > 1:\n",
    "            text_emb = text_emb.squeeze(0)\n",
    "        \n",
    "        # Load audio embedding (1024-dim)\n",
    "        audio_emb = torch.load(self.audio_paths[idx])\n",
    "        if audio_emb.dim() > 1:\n",
    "            audio_emb = audio_emb.squeeze(0)\n",
    "        \n",
    "        return video, mask, text_emb, audio_emb, self.labels[idx]\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# DATA PREPARATION\n",
    "# =============================================================================\n",
    "def match_video_text_pairs(video_paths, text_paths):\n",
    "    \"\"\"Match video files with corresponding text embeddings.\"\"\"\n",
    "    def get_base_key(video_path):\n",
    "        stem = Path(video_path).stem\n",
    "        parts = stem.split(\"_\")\n",
    "        return \"_\".join(parts[:-1])\n",
    "    \n",
    "    matched_pairs = []\n",
    "    for v in video_paths:\n",
    "        base = get_base_key(v)\n",
    "        matches = [t for t in text_paths if Path(t).stem.startswith(base)]\n",
    "        if matches:\n",
    "            matched_pairs.append((v, matches[0]))\n",
    "    \n",
    "    return matched_pairs\n",
    "\n",
    "\n",
    "def extract_event_label(video_path):\n",
    "    \"\"\"Extract event type from video filename.\"\"\"\n",
    "    stem = Path(video_path).stem\n",
    "    return stem.split(\"_\")[-1]\n",
    "\n",
    "\n",
    "def prepare_dataset(video_dir=\"fusion/embeddings 2\",\n",
    "                   text_dir=\"fusion/text_embeddings_events\",\n",
    "                   audio_dir=\"fusion/audio_embeddings_events\",\n",
    "                   max_seq_len=100):\n",
    "    \"\"\"Prepare matched dataset with labels.\"\"\"\n",
    "    \n",
    "    video_paths = sorted(glob(f\"{video_dir}/*.npy\"))\n",
    "    text_paths = sorted(glob(f\"{text_dir}/*.pt\"))\n",
    "    audio_paths = sorted(glob(f\"{audio_dir}/*.pt\"))\n",
    "    \n",
    "    print(f\"Found {len(video_paths)} video files\")\n",
    "    print(f\"Found {len(text_paths)} text files\")\n",
    "    print(f\"Found {len(audio_paths)} audio files\")\n",
    "    \n",
    "    # Match pairs\n",
    "    if Path(\"fusion/matched_pairs.json\").exists():\n",
    "        with open(\"fusion/matched_pairs.json\", \"r\") as f:\n",
    "            matched_pairs = [tuple(x) for x in json.load(f)]\n",
    "    else:\n",
    "        matched_pairs = match_video_text_pairs(video_paths, text_paths)\n",
    "        with open(\"fusion/matched_pairs.json\", \"w\") as f:\n",
    "            json.dump([(v, t) for v, t in matched_pairs], f)\n",
    "    \n",
    "    video_paths_matched = [v for v, t in matched_pairs]\n",
    "    text_paths_matched = [t for v, t in matched_pairs]\n",
    "    \n",
    "    # Match audio\n",
    "    def get_base_key(video_path):\n",
    "        stem = Path(video_path).stem\n",
    "        parts = stem.split(\"_\")\n",
    "        return \"_\".join(parts[:-1])\n",
    "    \n",
    "    audio_paths_matched = []\n",
    "    valid_indices = []\n",
    "    for idx, v in enumerate(video_paths_matched):\n",
    "        base = get_base_key(v)\n",
    "        audio_matches = [a for a in audio_paths if Path(a).stem.startswith(base)]\n",
    "        if audio_matches:\n",
    "            audio_paths_matched.append(audio_matches[0])\n",
    "            valid_indices.append(idx)\n",
    "    \n",
    "    video_paths_matched = [video_paths_matched[i] for i in valid_indices]\n",
    "    text_paths_matched = [text_paths_matched[i] for i in valid_indices]\n",
    "    \n",
    "    event_types = [extract_event_label(v) for v in video_paths_matched]\n",
    "    \n",
    "    unique_events = sorted(set(event_types))\n",
    "    event_to_idx = {ev: i for i, ev in enumerate(unique_events)}\n",
    "    labels = [event_to_idx[ev] for ev in event_types]\n",
    "    \n",
    "    print(f\"\\nMatched {len(video_paths_matched)} complete triplets\")\n",
    "    print(f\"Event classes: {len(unique_events)}\")\n",
    "    \n",
    "    return (video_paths_matched, text_paths_matched, audio_paths_matched,\n",
    "            labels, event_to_idx, max_seq_len)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# TRAINING FUNCTIONS\n",
    "# =============================================================================\n",
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for video, mask, text_emb, audio_emb, y in loader:\n",
    "        video = video.to(device)\n",
    "        mask = mask.to(device)\n",
    "        text_emb = text_emb.to(device)\n",
    "        audio_emb = audio_emb.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = model(video, mask, text_emb, audio_emb)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        correct += (logits.argmax(1) == y).sum().item()\n",
    "        total += y.size(0)\n",
    "    \n",
    "    return total_loss / len(loader), correct / total\n",
    "\n",
    "\n",
    "def validate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for video, mask, text_emb, audio_emb, y in loader:\n",
    "            video = video.to(device)\n",
    "            mask = mask.to(device)\n",
    "            text_emb = text_emb.to(device)\n",
    "            audio_emb = audio_emb.to(device)\n",
    "            y = y.to(device)\n",
    "            \n",
    "            logits = model(video, mask, text_emb, audio_emb)\n",
    "            loss = criterion(logits, y)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            correct += (logits.argmax(1) == y).sum().item()\n",
    "            total += y.size(0)\n",
    "    \n",
    "    return total_loss / len(loader), correct / total\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, epochs=30, lr=1e-3,\n",
    "                patience=15, device='cuda', model_name='model'):\n",
    "    model.to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0.05)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, T_max=30, eta_min=1e-6\n",
    "    )\n",
    "    \n",
    "    best_val_acc = 0\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), f'best_{model_name}.pt')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                break\n",
    "    \n",
    "    model.load_state_dict(torch.load(f'best_{model_name}.pt'))\n",
    "    _, final_acc = validate(model, val_loader, criterion, device)\n",
    "    return final_acc\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN EXECUTION\n",
    "# =============================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1e77008a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5820 video files\n",
      "Found 5832 text files\n",
      "Found 5832 audio files\n",
      "\n",
      "Matched 5816 complete triplets\n",
      "Event classes: 16\n",
      "\n",
      "Train: 4652 samples\n",
      "Val: 1164 samples\n",
      "\n",
      "======================================================================\n",
      "COMPLETE MULTIMODAL FUSION COMPARISON\n",
      "======================================================================\n",
      "\n",
      "[Training Video Only (V)]\n",
      "Parameters: 1,023,376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/ice1/0/8/mdoutre3/py311venv/lib/python3.11/site-packages/torch/nn/modules/transformer.py:515: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. We recommend specifying layout=torch.jagged when constructing a nested tensor, as this layout receives active development, has better operator coverage, and works with torch.compile. (Triggered internally at /pytorch/aten/src/ATen/NestedTensorImpl.cpp:178.)\n",
      "  output = torch._nested_tensor_from_mask(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video Only (V) Val Acc: 0.658\n",
      "\n",
      "[Training Text Only (T)]\n",
      "Parameters: 232,592\n",
      "Text Only (T) Val Acc: 0.527\n",
      "\n",
      "[Training Audio Only (A)]\n",
      "Parameters: 298,128\n",
      "Audio Only (A) Val Acc: 0.293\n",
      "\n",
      "[Training Video + Text (VT)]\n",
      "Parameters: 1,189,392\n",
      "Video + Text (VT) Val Acc: 0.747\n",
      "\n",
      "[Training Video + Audio (VA)]\n",
      "Parameters: 1,254,928\n",
      "Video + Audio (VA) Val Acc: 0.662\n",
      "\n",
      "[Training Text + Audio (TA)]\n",
      "Parameters: 626,320\n",
      "Text + Audio (TA) Val Acc: 0.516\n",
      "\n",
      "[Training Video + Text + Audio (VTA)]\n",
      "Parameters: 1,451,792\n",
      "Video + Text + Audio (VTA) Val Acc: 0.740\n",
      "\n",
      "======================================================================\n",
      "FINAL RESULTS SUMMARY\n",
      "======================================================================\n",
      "Model                          | Val Accuracy\n",
      "----------------------------------------------------------------------\n",
      "Video Only (V)                 | 0.658\n",
      "Text Only (T)                  | 0.527\n",
      "Audio Only (A)                 | 0.293\n",
      "Video + Text (VT)              | 0.747\n",
      "Video + Audio (VA)             | 0.662\n",
      "Text + Audio (TA)              | 0.516\n",
      "Video + Text + Audio (VTA)     | 0.740\n",
      "======================================================================\n",
      "\n",
      "Best single modality: 0.658\n",
      "Best bimodal fusion: 0.747\n",
      "Trimodal fusion: 0.740\n",
      "\n",
      "Bimodal improvement over single: +0.089\n",
      "Trimodal improvement over bimodal: +-0.008\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Prepare data\n",
    "    (video_paths, text_paths, audio_paths, labels,\n",
    "     event_to_idx, max_seq_len) = prepare_dataset(max_seq_len=100)\n",
    "    \n",
    "    # Split indices\n",
    "    indices = list(range(len(labels)))\n",
    "    np.random.seed(42)\n",
    "    np.random.shuffle(indices)\n",
    "    train_size = int(0.8 * len(indices))\n",
    "    train_indices = indices[:train_size]\n",
    "    val_indices = indices[train_size:]\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = SoccerTrimodalDataset(\n",
    "        video_paths=[video_paths[i] for i in train_indices],\n",
    "        text_paths=[text_paths[i] for i in train_indices],\n",
    "        audio_paths=[audio_paths[i] for i in train_indices],\n",
    "        labels=[labels[i] for i in train_indices],\n",
    "        max_seq_len=max_seq_len\n",
    "    )\n",
    "    \n",
    "    val_dataset = SoccerTrimodalDataset(\n",
    "        video_paths=[video_paths[i] for i in val_indices],\n",
    "        text_paths=[text_paths[i] for i in val_indices],\n",
    "        audio_paths=[audio_paths[i] for i in val_indices],\n",
    "        labels=[labels[i] for i in val_indices],\n",
    "        max_seq_len=max_seq_len\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True,\n",
    "                             num_workers=2, pin_memory=True, drop_last=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False,\n",
    "                           num_workers=2, pin_memory=True)\n",
    "    \n",
    "    print(f\"\\nTrain: {len(train_dataset)} samples\")\n",
    "    print(f\"Val: {len(val_dataset)} samples\")\n",
    "    \n",
    "    # Define all experiments\n",
    "    experiments = [\n",
    "        (\"Video Only (V)\", VideoOnlyModel(num_classes=len(event_to_idx), dropout=0.1)),\n",
    "        (\"Text Only (T)\", TextOnlyModel(num_classes=len(event_to_idx), dropout=0.1)),\n",
    "        (\"Audio Only (A)\", AudioOnlyModel(num_classes=len(event_to_idx), dropout=0.1)),\n",
    "        (\"Video + Text (VT)\", VideoTextFusion(num_classes=len(event_to_idx), dropout=0.1)),\n",
    "        (\"Video + Audio (VA)\", VideoAudioFusion(num_classes=len(event_to_idx), dropout=0.1)),\n",
    "        (\"Text + Audio (TA)\", TextAudioFusion(num_classes=len(event_to_idx), dropout=0.1)),\n",
    "        (\"Video + Text + Audio (VTA)\", TrimodalFusion(num_classes=len(event_to_idx), dropout=0.1)),\n",
    "    ]\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"COMPLETE MULTIMODAL FUSION COMPARISON\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    for name, model in experiments:\n",
    "        print(f\"\\n[Training {name}]\")\n",
    "        num_params = sum(p.numel() for p in model.parameters())\n",
    "        print(f\"Parameters: {num_params:,}\")\n",
    "        \n",
    "        model_key = name.split('(')[1].strip(')')\n",
    "        val_acc = train_model(\n",
    "            model, train_loader, val_loader,\n",
    "            epochs=30, lr=1e-3, patience=15,\n",
    "            device=device, model_name=model_key\n",
    "        )\n",
    "        \n",
    "        results[name] = val_acc\n",
    "        print(f\"{name} Val Acc: {val_acc:.3f}\")\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"FINAL RESULTS SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"{'Model':<30} | {'Val Accuracy'}\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    for name in [\"Video Only (V)\", \"Text Only (T)\", \"Audio Only (A)\",\n",
    "                 \"Video + Text (VT)\", \"Video + Audio (VA)\", \"Text + Audio (TA)\",\n",
    "                 \"Video + Text + Audio (VTA)\"]:\n",
    "        print(f\"{name:<30} | {results[name]:.3f}\")\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Calculate improvements\n",
    "    best_single = max(results[\"Video Only (V)\"], results[\"Text Only (T)\"], results[\"Audio Only (A)\"])\n",
    "    best_bimodal = max(results[\"Video + Text (VT)\"], results[\"Video + Audio (VA)\"], results[\"Text + Audio (TA)\"])\n",
    "    trimodal = results[\"Video + Text + Audio (VTA)\"]\n",
    "    \n",
    "    print(f\"\\nBest single modality: {best_single:.3f}\")\n",
    "    print(f\"Best bimodal fusion: {best_bimodal:.3f}\")\n",
    "    print(f\"Trimodal fusion: {trimodal:.3f}\")\n",
    "    print(f\"\\nBimodal improvement over single: +{(best_bimodal - best_single):.3f}\")\n",
    "    print(f\"Trimodal improvement over bimodal: +{(trimodal - best_bimodal):.3f}\")\n",
    "    print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b1b8894c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing audio file: fusion/audio_embeddings_events/2014-11-05 - 22-45 Bayern Munich 2 - 0 AS Roma_1_224p_2709.00.pt\n",
      "\n",
      "Type: <class 'torch.Tensor'>\n",
      "Direct tensor shape: torch.Size([1024])\n",
      "Direct tensor dtype: torch.float32\n",
      "\n",
      "\n",
      "For comparison, testing text file: fusion/text_embeddings_events/2014-11-05 - 22-45 Bayern Munich 2 - 0 AS Roma_1_224p_2709.00.pt\n",
      "Type: <class 'dict'>\n",
      "Dict keys: dict_keys(['match_name', 'event_key', 'embedding', 'label', 'timestamp', 'audio_path'])\n",
      "Embedding shape: torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "\n",
    "# Get one audio file\n",
    "audio_paths = sorted(glob(\"fusion/audio_embeddings_events/*.pt\"))\n",
    "\n",
    "if audio_paths:\n",
    "    print(f\"Testing audio file: {audio_paths[0]}\")\n",
    "    audio_data = torch.load(audio_paths[0])\n",
    "    \n",
    "    print(f\"\\nType: {type(audio_data)}\")\n",
    "    \n",
    "    if isinstance(audio_data, dict):\n",
    "        print(f\"Dict keys: {audio_data.keys()}\")\n",
    "        if \"embedding\" in audio_data:\n",
    "            emb = audio_data[\"embedding\"]\n",
    "            print(f\"Embedding shape: {emb.shape}\")\n",
    "            print(f\"Embedding dtype: {emb.dtype}\")\n",
    "    elif isinstance(audio_data, torch.Tensor):\n",
    "        print(f\"Direct tensor shape: {audio_data.shape}\")\n",
    "        print(f\"Direct tensor dtype: {audio_data.dtype}\")\n",
    "    else:\n",
    "        print(f\"Unknown format: {audio_data}\")\n",
    "    \n",
    "    # Compare with text file format\n",
    "    text_paths = sorted(glob(\"fusion/text_embeddings_events/*.pt\"))\n",
    "    if text_paths:\n",
    "        print(f\"\\n\\nFor comparison, testing text file: {text_paths[0]}\")\n",
    "        text_data = torch.load(text_paths[0])\n",
    "        print(f\"Type: {type(text_data)}\")\n",
    "        if isinstance(text_data, dict):\n",
    "            print(f\"Dict keys: {text_data.keys()}\")\n",
    "            if \"embedding\" in text_data:\n",
    "                emb = text_data[\"embedding\"]\n",
    "                print(f\"Embedding shape: {emb.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a9d266e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best single modality: 0.658\n",
      "Best bimodal fusion: 0.747\n",
      "Trimodal fusion: 0.740\n",
      "\n",
      "Bimodal improvement over single: +0.089\n",
      "Trimodal improvement over bimodal: +-0.008\n",
      "======================================================================\n",
      "\n",
      "Saved results to: fusion/experiment_results/trimodal_fusion_results_20251130-224251.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "\n",
    "print(f\"\\nBest single modality: {best_single:.3f}\")\n",
    "print(f\"Best bimodal fusion: {best_bimodal:.3f}\")\n",
    "print(f\"Trimodal fusion: {trimodal:.3f}\")\n",
    "print(f\"\\nBimodal improvement over single: +{(best_bimodal - best_single):.3f}\")\n",
    "print(f\"Trimodal improvement over bimodal: +{(trimodal - best_bimodal):.3f}\")\n",
    "print(\"=\"*70)\n",
    "# ---------- SAVE RESULTS TO FILE ----------\n",
    "# Build a summary dict\n",
    "results_summary = {\n",
    "    \"results_per_model\": results,  # e.g. {\"Video Only (V)\": 0.73, ...}\n",
    "    \"best_single\": float(best_single),\n",
    "    \"best_bimodal\": float(best_bimodal),\n",
    "    \"trimodal\": float(trimodal),\n",
    "    \"bimodal_minus_single\": float(best_bimodal - best_single),\n",
    "    \"trimodal_minus_bimodal\": float(trimodal - best_bimodal),\n",
    "    \"num_train_samples\": len(train_dataset),\n",
    "    \"num_val_samples\": len(val_dataset),\n",
    "    \"timestamp\": datetime.now().isoformat(timespec=\"seconds\")\n",
    "}\n",
    "# Output folder + filename\n",
    "out_dir = Path(\"fusion/experiment_results\")\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "out_path = out_dir / f\"trimodal_fusion_results_{timestamp}.json\"\n",
    "with out_path.open(\"w\") as f:\n",
    "    json.dump(results_summary, f, indent=2)\n",
    "print(f\"\\nSaved results to: {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1959a475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Training Video Only (V)]\n",
      "Parameters: 1,023,376\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable float object",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m model_key = name.split(\u001b[33m'\u001b[39m\u001b[33m(\u001b[39m\u001b[33m'\u001b[39m)[\u001b[32m1\u001b[39m].strip(\u001b[33m'\u001b[39m\u001b[33m)\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# ↓ assume train_model now returns (val_acc, per_class_vals)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m val_acc, per_class_vals = train_model(\n\u001b[32m     12\u001b[39m     model, train_loader, val_loader,\n\u001b[32m     13\u001b[39m     epochs=\u001b[32m30\u001b[39m, lr=\u001b[32m1e-3\u001b[39m, patience=\u001b[32m15\u001b[39m,\n\u001b[32m     14\u001b[39m     device=device, model_name=model_key\n\u001b[32m     15\u001b[39m )\n\u001b[32m     17\u001b[39m results[name] = val_acc\n\u001b[32m     18\u001b[39m per_class_metrics[model_key] = per_class_vals  \u001b[38;5;66;03m# shape: [num_classes]\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: cannot unpack non-iterable float object"
     ]
    }
   ],
   "source": [
    "per_class_metrics = {}  # model_key -> per-class metric vector\n",
    "\n",
    "for name, model in experiments:\n",
    "    print(f\"\\n[Training {name}]\")\n",
    "    num_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Parameters: {num_params:,}\")\n",
    "    \n",
    "    model_key = name.split('(')[1].strip(')')\n",
    "    \n",
    "    # ↓ assume train_model now returns (val_acc, per_class_vals)\n",
    "    val_acc, per_class_vals = train_model(\n",
    "        model, train_loader, val_loader,\n",
    "        epochs=30, lr=1e-3, patience=15,\n",
    "        device=device, model_name=model_key\n",
    "    )\n",
    "    \n",
    "    results[name] = val_acc\n",
    "    per_class_metrics[model_key] = per_class_vals  # shape: [num_classes]\n",
    "    print(f\"{name} Val Acc: {val_acc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ec6299f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "PER-CLASS VALIDATION METRICS BY MODEL\n",
      "======================================================================\n",
      "Class                             V         T         A        VT        VA        TA       VTA\n",
      "-----------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'per_class_metrics' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     21\u001b[39m row = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcls_name\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m<25\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m mk \u001b[38;5;129;01min\u001b[39;00m model_keys:\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m     val = \u001b[43mper_class_metrics\u001b[49m[mk][i]\n\u001b[32m     24\u001b[39m     val = \u001b[38;5;28mfloat\u001b[39m(val)  \u001b[38;5;66;03m# in case it's a tensor\u001b[39;00m\n\u001b[32m     25\u001b[39m     row += \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m10.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m: name 'per_class_metrics' is not defined"
     ]
    }
   ],
   "source": [
    "    # ---------- PER-CLASS TABLE ----------\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"PER-CLASS VALIDATION METRICS BY MODEL\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    # Invert event_to_idx to get ordered class names\n",
    "    idx_to_event = {idx: cls for cls, idx in event_to_idx.items()}\n",
    "    num_classes = len(idx_to_event)\n",
    "    class_names = [idx_to_event[i] for i in range(num_classes)]\n",
    "\n",
    "    # Column order (must match keys in per_class_metrics)\n",
    "    model_keys = [\"V\", \"T\", \"A\", \"VT\", \"VA\", \"TA\", \"VTA\"]\n",
    "\n",
    "    # Header\n",
    "    header = f\"{'Class':<25}\" + \"\".join(f\"{mk:>10}\" for mk in model_keys)\n",
    "    print(header)\n",
    "    print(\"-\" * len(header))\n",
    "\n",
    "    # Rows: one per event class\n",
    "    for i, cls_name in enumerate(class_names):\n",
    "        row = f\"{cls_name:<25}\"\n",
    "        for mk in model_keys:\n",
    "            val = per_class_metrics[mk][i]\n",
    "            val = float(val)  # in case it's a tensor\n",
    "            row += f\"{val:10.3f}\"\n",
    "        print(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4bd24b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (GPU311)",
   "language": "python",
   "name": "gpu311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
