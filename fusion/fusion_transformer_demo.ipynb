{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5292fd56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hice1/mdoutre3/CS7643_Project_1\n"
     ]
    }
   ],
   "source": [
    "%cd /home/hice1/mdoutre3/CS7643_Project_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad771f79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from glob import glob\n",
    "import math\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Running on: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1aa77a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d0a050",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# MODEL COMPONENTS\n",
    "# =============================================================================\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"Positional encoding for temporal sequences.\"\"\"\n",
    "    def __init__(self, d_model, max_len=600, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() *\n",
    "                             (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class MultimodalFusionTransformer(nn.Module):\n",
    "    def __init__(self, input_dim=768, d_model=256, nhead=4, num_layers=2,\n",
    "                 num_classes=15, dropout=0.1, max_seq_len=100):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Much simpler projection with heavy dropout\n",
    "        self.input_proj = nn.Sequential(\n",
    "            nn.Linear(input_dim, d_model),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        self.pos_encoder = PositionalEncoding(d_model, max_seq_len + 2, dropout=dropout)\n",
    "        \n",
    "        # Single lightweight transformer layer\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=d_model,  # Same size as d_model (minimal capacity)\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "            norm_first=True  # Pre-norm for better training\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "        \n",
    "        # Simple classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, video_x, video_mask, text_emb):\n",
    "        \"\"\"\n",
    "        video_x: (B, T, 768)\n",
    "        video_mask: (B, T) - True for valid positions\n",
    "        text_emb: (B, 768)\n",
    "        \"\"\"\n",
    "        B = video_x.size(0)\n",
    "        \n",
    "        # Project inputs\n",
    "        video_x = self.input_proj(video_x)\n",
    "        text_x = self.input_proj(text_emb).unsqueeze(1)\n",
    "        \n",
    "        # Concatenate text as CLS token\n",
    "        x = torch.cat([text_x, video_x], dim=1)  # (B, T+1, d_model)\n",
    "        \n",
    "        # Create full mask\n",
    "        text_mask = torch.ones(B, 1, dtype=torch.bool, device=video_mask.device)\n",
    "        full_mask = torch.cat([text_mask, video_mask], dim=1)\n",
    "        \n",
    "        # Add positional encoding\n",
    "        x = self.pos_encoder(x)\n",
    "        \n",
    "        # Transformer encoding\n",
    "        x = self.encoder(x, src_key_padding_mask=~full_mask)\n",
    "        \n",
    "        # Use CLS token for classification\n",
    "        cls = x[:, 0]\n",
    "        return self.classifier(cls)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# AUGMENTED DATASET WITH MIXUP\n",
    "# =============================================================================\n",
    "\n",
    "class SoccerFusionDataset(Dataset):\n",
    "    \"\"\"Dataset for video+text fusion using matched pairs.\"\"\"\n",
    "    \n",
    "    def __init__(self, video_paths, text_paths, labels, max_seq_len=100, \n",
    "                 augment=False):\n",
    "        self.video_paths = video_paths\n",
    "        self.text_paths = text_paths\n",
    "        self.labels = labels\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.augment = augment\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.video_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Load video features\n",
    "        video = np.load(self.video_paths[idx])  # (T, 768)\n",
    "        video = torch.from_numpy(video).float()\n",
    "        T = video.shape[0]\n",
    "        \n",
    "        # Random temporal augmentation during training\n",
    "        if self.augment and T > 20:\n",
    "            # Randomly drop up to 30% of frames\n",
    "            keep_ratio = np.random.uniform(0.7, 1.0)\n",
    "            keep_frames = max(20, int(T * keep_ratio))\n",
    "            start_idx = np.random.randint(0, T - keep_frames + 1)\n",
    "            video = video[start_idx:start_idx + keep_frames]\n",
    "            T = keep_frames\n",
    "        \n",
    "        # Pad or truncate video\n",
    "        if T > self.max_seq_len:\n",
    "            # Random crop instead of always taking first frames\n",
    "            if self.augment:\n",
    "                start = np.random.randint(0, T - self.max_seq_len + 1)\n",
    "                video = video[start:start + self.max_seq_len]\n",
    "            else:\n",
    "                video = video[:self.max_seq_len]\n",
    "            mask = torch.ones(self.max_seq_len, dtype=torch.bool)\n",
    "        else:\n",
    "            pad = torch.zeros(self.max_seq_len - T, 768)\n",
    "            video = torch.cat([video, pad], dim=0)\n",
    "            mask = torch.cat([\n",
    "                torch.ones(T, dtype=torch.bool),\n",
    "                torch.zeros(self.max_seq_len - T, dtype=torch.bool)\n",
    "            ])\n",
    "        \n",
    "        # Add Gaussian noise for augmentation\n",
    "        if self.augment:\n",
    "            video = video + torch.randn_like(video) * 0.01\n",
    "        \n",
    "        # Load text embedding\n",
    "        text_data = torch.load(self.text_paths[idx])\n",
    "        text_emb = text_data[\"embedding\"].squeeze(0)  # (768,)\n",
    "        \n",
    "        if self.augment:\n",
    "            text_emb = text_emb + torch.randn_like(text_emb) * 0.01\n",
    "        \n",
    "        return video, mask, text_emb, self.labels[idx]\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# DATA LOADING & PREPROCESSING\n",
    "# =============================================================================\n",
    "\n",
    "def match_video_text_pairs(video_paths, text_paths):\n",
    "    \"\"\"Match video files with corresponding text embeddings.\"\"\"\n",
    "    text_lookup = {Path(t).stem: t for t in text_paths}\n",
    "    \n",
    "    def get_base_key(video_path):\n",
    "        stem = Path(video_path).stem\n",
    "        parts = stem.split(\"_\")\n",
    "        return \"_\".join(parts[:-1])  # Remove event name\n",
    "    \n",
    "    matched_pairs = []\n",
    "    for v in video_paths:\n",
    "        base = get_base_key(v)\n",
    "        matches = [t for t in text_paths if Path(t).stem.startswith(base)]\n",
    "        if matches:\n",
    "            matched_pairs.append((v, matches[0]))\n",
    "    \n",
    "    return matched_pairs\n",
    "\n",
    "\n",
    "def extract_event_label(video_path):\n",
    "    \"\"\"Extract event type from video filename.\"\"\"\n",
    "    stem = Path(video_path).stem\n",
    "    return stem.split(\"_\")[-1]\n",
    "\n",
    "\n",
    "def prepare_dataset(video_dir=\"fusion/embeddings 2\", \n",
    "                   text_dir=\"fusion/text_embeddings_events\",\n",
    "                   max_seq_len=100):\n",
    "    \"\"\"Prepare matched dataset with labels.\"\"\"\n",
    "    \n",
    "    video_paths = sorted(glob(f\"{video_dir}/*.npy\"))\n",
    "    text_paths = sorted(glob(f\"{text_dir}/*.pt\"))\n",
    "    \n",
    "    print(f\"Found {len(video_paths)} video files\")\n",
    "    print(f\"Found {len(text_paths)} text files\")\n",
    "    \n",
    "    # Match pairs\n",
    "    #matched_pairs = match_video_text_pairs(video_paths, text_paths)\n",
    "    \n",
    "\n",
    "    if Path(\"fusion/matched_pairs.json\").exists():\n",
    "        print(\"Loading cached matched pairs...\")\n",
    "        with open(\"fusion/matched_pairs.json\", \"r\") as f:\n",
    "            matched_pairs = [tuple(x) for x in json.load(f)]\n",
    "\n",
    "    else:\n",
    "        print(\"Computing matched pairs ...\")\n",
    "        matched_pairs = match_video_text_pairs(video_paths, text_paths)\n",
    "        print(f\"Matched {len(matched_pairs)} pairs\")\n",
    "        \n",
    "        print(\"Saving matched pairs...\")\n",
    "        with open(\"fusion/matched_pairs.json\", \"w\") as f:\n",
    "            json.dump([(v, t) for v, t in matched_pairs], f)\n",
    "\n",
    "    \n",
    "    # Extract labels\n",
    "    video_paths_matched = [v for v, t in matched_pairs]\n",
    "    text_paths_matched = [t for v, t in matched_pairs]\n",
    "    event_types = [extract_event_label(v) for v in video_paths_matched]\n",
    "    \n",
    "    # Create label mapping\n",
    "    unique_events = sorted(set(event_types))\n",
    "    event_to_idx = {ev: i for i, ev in enumerate(unique_events)}\n",
    "    labels = [event_to_idx[ev] for ev in event_types]\n",
    "    \n",
    "    print(f\"\\nEvent classes ({len(unique_events)}):\")\n",
    "    class_counts = {}\n",
    "    for ev, idx in event_to_idx.items():\n",
    "        count = sum(1 for l in labels if l == idx)\n",
    "        class_counts[idx] = count\n",
    "        print(f\"  {idx:2d}: {ev:25s} ({count} samples)\")\n",
    "    \n",
    "    # Return the base data and metadata\n",
    "    return (video_paths_matched, text_paths_matched, labels, \n",
    "            event_to_idx, class_counts, max_seq_len)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# TRAINING WITH MIXUP\n",
    "# =============================================================================\n",
    "\n",
    "def mixup_data(x_video, x_mask, x_text, y, alpha=0.2):\n",
    "    \"\"\"Mixup augmentation.\"\"\"\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "\n",
    "    batch_size = x_video.size(0)\n",
    "    index = torch.randperm(batch_size, device=x_video.device)\n",
    "\n",
    "    mixed_video = lam * x_video + (1 - lam) * x_video[index]\n",
    "    mixed_text = lam * x_text + (1 - lam) * x_text[index]\n",
    "    # Keep original mask for simplicity\n",
    "    \n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_video, x_mask, mixed_text, y_a, y_b, lam\n",
    "\n",
    "\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    \"\"\"Mixup loss calculation.\"\"\"\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
    "\n",
    "\n",
    "def train_epoch(model, loader, criterion, optimizer, device, use_mixup=True):\n",
    "    \"\"\"Train for one epoch with optional mixup.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for video, mask, text_emb, y in loader:\n",
    "        video = video.to(device)\n",
    "        mask = mask.to(device)\n",
    "        text_emb = text_emb.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Apply mixup with 50% probability\n",
    "        if use_mixup and np.random.rand() > 0.5:\n",
    "            video, mask, text_emb, y_a, y_b, lam = mixup_data(\n",
    "                video, mask, text_emb, y, alpha=0.2\n",
    "            )\n",
    "            logits = model(video, mask, text_emb)\n",
    "            loss = mixup_criterion(criterion, logits, y_a, y_b, lam)\n",
    "        else:\n",
    "            logits = model(video, mask, text_emb)\n",
    "            loss = criterion(logits, y)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        correct += (logits.argmax(1) == y).sum().item()\n",
    "        total += y.size(0)\n",
    "    \n",
    "    return total_loss / len(loader), correct / total\n",
    "\n",
    "\n",
    "def validate(model, loader, criterion, device):\n",
    "    \"\"\"Validate model.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for video, mask, text_emb, y in loader:\n",
    "            video = video.to(device)\n",
    "            mask = mask.to(device)\n",
    "            text_emb = text_emb.to(device)\n",
    "            y = y.to(device)\n",
    "            \n",
    "            logits = model(video, mask, text_emb)\n",
    "            loss = criterion(logits, y)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            correct += (logits.argmax(1) == y).sum().item()\n",
    "            total += y.size(0)\n",
    "    \n",
    "    return total_loss / len(loader), correct / total\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, epochs=100, lr=5e-4, \n",
    "                patience=15, device='cuda', class_counts=None):\n",
    "    \"\"\"Train with early stopping and class balancing.\"\"\"\n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    # Weighted loss for class imbalance\n",
    "    if class_counts:\n",
    "        # Correct way to extract class frequencies\n",
    "        freqs = [class_counts[i] for i in range(len(class_counts))]\n",
    "\n",
    "        weights = torch.tensor(freqs, dtype=torch.float)\n",
    "        weights = 1.0 / weights            # inverse frequency\n",
    "        weights = weights / weights.sum()  # normalize\n",
    "        weights = weights.to(device)\n",
    "\n",
    "        #criterion = nn.CrossEntropyLoss(weight=weights, label_smoothing=0.1)\n",
    "        criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "    else:\n",
    "        criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0.05)\n",
    "    \n",
    "    # Cosine annealing with warm restarts\n",
    "    #scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "    #    optimizer, T_0=10, T_mult=2, eta_min=1e-6\n",
    "    #)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer,\n",
    "        T_max=30,  # the total number of epochs\n",
    "        eta_min=1e-6\n",
    "    )\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        train_loss, train_acc = train_epoch(\n",
    "            model, train_loader, criterion, optimizer, device, use_mixup=False\n",
    "        )\n",
    "        val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "        \n",
    "        print(f\"[Epoch {epoch+1:03d}] \"\n",
    "              f\"Train Loss={train_loss:.4f}, Acc={train_acc:.3f} | \"\n",
    "              f\"Val Loss={val_loss:.4f}, Acc={val_acc:.3f} | \"\n",
    "              f\"LR={optimizer.param_groups[0]['lr']:.2e}\")\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        # Early stopping based on validation loss\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), 'best_model.pt')\n",
    "            print(f\"  → Saved best model (val_loss={val_loss:.4f})\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"\\nEarly stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "    \n",
    "    # Load best model\n",
    "    model.load_state_dict(torch.load('best_model.pt'))\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac5b88c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5820 video files\n",
      "Found 5832 text files\n",
      "Loading cached matched pairs...\n",
      "\n",
      "Event classes (16):\n",
      "   0: Ball out of play          (1676 samples)\n",
      "   1: Clearance                 (433 samples)\n",
      "   2: Corner                    (249 samples)\n",
      "   3: Direct free-kick          (105 samples)\n",
      "   4: Foul                      (611 samples)\n",
      "   5: Goal                      (118 samples)\n",
      "   6: Indirect free-kick        (487 samples)\n",
      "   7: Kick-off                  (136 samples)\n",
      "   8: Offside                   (140 samples)\n",
      "   9: Penalty                   (6 samples)\n",
      "  10: Red card                  (7 samples)\n",
      "  11: Shots off target          (265 samples)\n",
      "  12: Shots on target           (299 samples)\n",
      "  13: Substitution              (146 samples)\n",
      "  14: Throw-in                  (1017 samples)\n",
      "  15: Yellow card               (121 samples)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/ice1/0/8/mdoutre3/py311venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train: 4652 samples (145 batches)\n",
      "Val:   1164 samples (37 batches)\n",
      "\n",
      "======================================================================\n",
      "BASELINE EXPERIMENTS\n",
      "======================================================================\n",
      "\n",
      "[1/3] Training TEXT-ONLY baseline...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/ice1/0/8/mdoutre3/py311venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 001] Train Loss=1.9528, Acc=0.451 | Val Loss=1.7545, Acc=0.500 | LR=1.00e-03\n",
      "  → Saved best model (val_loss=1.7545)\n",
      "[Epoch 002] Train Loss=1.6858, Acc=0.541 | Val Loss=1.7462, Acc=0.489 | LR=9.97e-04\n",
      "  → Saved best model (val_loss=1.7462)\n",
      "[Epoch 003] Train Loss=1.6095, Acc=0.574 | Val Loss=1.7354, Acc=0.515 | LR=9.89e-04\n",
      "  → Saved best model (val_loss=1.7354)\n",
      "[Epoch 004] Train Loss=1.5171, Acc=0.608 | Val Loss=1.7654, Acc=0.517 | LR=9.76e-04\n",
      "[Epoch 005] Train Loss=1.4145, Acc=0.649 | Val Loss=1.8337, Acc=0.487 | LR=9.57e-04\n",
      "[Epoch 006] Train Loss=1.3090, Acc=0.701 | Val Loss=1.9198, Acc=0.486 | LR=9.33e-04\n",
      "[Epoch 007] Train Loss=1.2110, Acc=0.748 | Val Loss=1.9650, Acc=0.494 | LR=9.05e-04\n",
      "[Epoch 008] Train Loss=1.1199, Acc=0.794 | Val Loss=2.0060, Acc=0.496 | LR=8.72e-04\n",
      "[Epoch 009] Train Loss=1.0219, Acc=0.835 | Val Loss=2.1035, Acc=0.493 | LR=8.35e-04\n",
      "[Epoch 010] Train Loss=0.9693, Acc=0.856 | Val Loss=2.1662, Acc=0.475 | LR=7.94e-04\n",
      "[Epoch 011] Train Loss=0.9153, Acc=0.882 | Val Loss=2.1761, Acc=0.486 | LR=7.50e-04\n",
      "[Epoch 012] Train Loss=0.8635, Acc=0.903 | Val Loss=2.2466, Acc=0.474 | LR=7.04e-04\n",
      "[Epoch 013] Train Loss=0.8322, Acc=0.916 | Val Loss=2.2431, Acc=0.482 | LR=6.55e-04\n",
      "\n",
      "Early stopping at epoch 13\n",
      "TEXT-ONLY Val Acc: 0.515\n",
      "\n",
      "[2/3] Training VIDEO-ONLY baseline...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/ice1/0/8/mdoutre3/py311venv/lib/python3.11/site-packages/torch/nn/modules/transformer.py:515: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. We recommend specifying layout=torch.jagged when constructing a nested tensor, as this layout receives active development, has better operator coverage, and works with torch.compile. (Triggered internally at /pytorch/aten/src/ATen/NestedTensorImpl.cpp:178.)\n",
      "  output = torch._nested_tensor_from_mask(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 001] Train Loss=2.2441, Acc=0.340 | Val Loss=2.0704, Acc=0.405 | LR=1.00e-03\n",
      "  → Saved best model (val_loss=2.0704)\n",
      "[Epoch 002] Train Loss=1.8682, Acc=0.440 | Val Loss=1.9057, Acc=0.415 | LR=9.97e-04\n",
      "  → Saved best model (val_loss=1.9057)\n",
      "[Epoch 003] Train Loss=1.7074, Acc=0.500 | Val Loss=1.8079, Acc=0.487 | LR=9.89e-04\n",
      "  → Saved best model (val_loss=1.8079)\n",
      "[Epoch 004] Train Loss=1.6458, Acc=0.531 | Val Loss=1.7626, Acc=0.524 | LR=9.76e-04\n",
      "  → Saved best model (val_loss=1.7626)\n",
      "[Epoch 005] Train Loss=1.5834, Acc=0.558 | Val Loss=1.7508, Acc=0.536 | LR=9.57e-04\n",
      "  → Saved best model (val_loss=1.7508)\n",
      "[Epoch 006] Train Loss=1.5548, Acc=0.569 | Val Loss=1.6981, Acc=0.536 | LR=9.33e-04\n",
      "  → Saved best model (val_loss=1.6981)\n",
      "[Epoch 007] Train Loss=1.5081, Acc=0.598 | Val Loss=1.6798, Acc=0.549 | LR=9.05e-04\n",
      "  → Saved best model (val_loss=1.6798)\n",
      "[Epoch 008] Train Loss=1.4689, Acc=0.609 | Val Loss=1.6354, Acc=0.582 | LR=8.72e-04\n",
      "  → Saved best model (val_loss=1.6354)\n",
      "[Epoch 009] Train Loss=1.4350, Acc=0.625 | Val Loss=1.6391, Acc=0.576 | LR=8.35e-04\n",
      "[Epoch 010] Train Loss=1.4045, Acc=0.647 | Val Loss=1.6377, Acc=0.607 | LR=7.94e-04\n",
      "[Epoch 011] Train Loss=1.3761, Acc=0.651 | Val Loss=1.5955, Acc=0.619 | LR=7.50e-04\n",
      "  → Saved best model (val_loss=1.5955)\n",
      "[Epoch 012] Train Loss=1.3606, Acc=0.666 | Val Loss=1.5775, Acc=0.604 | LR=7.04e-04\n",
      "  → Saved best model (val_loss=1.5775)\n",
      "[Epoch 013] Train Loss=1.3242, Acc=0.679 | Val Loss=1.5935, Acc=0.601 | LR=6.55e-04\n",
      "[Epoch 014] Train Loss=1.3028, Acc=0.688 | Val Loss=1.5950, Acc=0.618 | LR=6.04e-04\n",
      "[Epoch 015] Train Loss=1.2726, Acc=0.701 | Val Loss=1.5309, Acc=0.624 | LR=5.53e-04\n",
      "  → Saved best model (val_loss=1.5309)\n",
      "[Epoch 016] Train Loss=1.2516, Acc=0.716 | Val Loss=1.5642, Acc=0.619 | LR=5.01e-04\n",
      "[Epoch 017] Train Loss=1.2236, Acc=0.722 | Val Loss=1.5445, Acc=0.627 | LR=4.48e-04\n",
      "[Epoch 018] Train Loss=1.1856, Acc=0.751 | Val Loss=1.5386, Acc=0.637 | LR=3.97e-04\n",
      "[Epoch 019] Train Loss=1.1600, Acc=0.756 | Val Loss=1.5412, Acc=0.630 | LR=3.46e-04\n",
      "[Epoch 020] Train Loss=1.1304, Acc=0.769 | Val Loss=1.5203, Acc=0.635 | LR=2.97e-04\n",
      "  → Saved best model (val_loss=1.5203)\n",
      "[Epoch 021] Train Loss=1.1100, Acc=0.775 | Val Loss=1.5797, Acc=0.608 | LR=2.51e-04\n",
      "[Epoch 022] Train Loss=1.0846, Acc=0.789 | Val Loss=1.5033, Acc=0.643 | LR=2.07e-04\n",
      "  → Saved best model (val_loss=1.5033)\n",
      "[Epoch 023] Train Loss=1.0617, Acc=0.799 | Val Loss=1.5146, Acc=0.637 | LR=1.66e-04\n",
      "[Epoch 024] Train Loss=1.0461, Acc=0.811 | Val Loss=1.4965, Acc=0.641 | LR=1.29e-04\n",
      "  → Saved best model (val_loss=1.4965)\n",
      "[Epoch 025] Train Loss=1.0261, Acc=0.815 | Val Loss=1.4926, Acc=0.644 | LR=9.64e-05\n",
      "  → Saved best model (val_loss=1.4926)\n",
      "[Epoch 026] Train Loss=1.0090, Acc=0.823 | Val Loss=1.4976, Acc=0.637 | LR=6.79e-05\n",
      "[Epoch 027] Train Loss=0.9943, Acc=0.827 | Val Loss=1.4994, Acc=0.640 | LR=4.42e-05\n",
      "[Epoch 028] Train Loss=0.9860, Acc=0.836 | Val Loss=1.4970, Acc=0.646 | LR=2.54e-05\n",
      "[Epoch 029] Train Loss=0.9756, Acc=0.839 | Val Loss=1.4973, Acc=0.649 | LR=1.19e-05\n",
      "[Epoch 030] Train Loss=0.9751, Acc=0.840 | Val Loss=1.4957, Acc=0.650 | LR=3.74e-06\n",
      "[Epoch 031] Train Loss=0.9749, Acc=0.841 | Val Loss=1.4954, Acc=0.651 | LR=1.00e-06\n",
      "[Epoch 032] Train Loss=0.9724, Acc=0.841 | Val Loss=1.4955, Acc=0.652 | LR=3.74e-06\n",
      "[Epoch 033] Train Loss=0.9732, Acc=0.838 | Val Loss=1.4960, Acc=0.647 | LR=1.19e-05\n",
      "[Epoch 034] Train Loss=0.9750, Acc=0.839 | Val Loss=1.4971, Acc=0.647 | LR=2.54e-05\n",
      "[Epoch 035] Train Loss=0.9820, Acc=0.838 | Val Loss=1.4975, Acc=0.649 | LR=4.42e-05\n",
      "[Epoch 036] Train Loss=0.9813, Acc=0.840 | Val Loss=1.4991, Acc=0.640 | LR=6.79e-05\n",
      "[Epoch 037] Train Loss=0.9839, Acc=0.836 | Val Loss=1.5129, Acc=0.643 | LR=9.64e-05\n",
      "[Epoch 038] Train Loss=0.9856, Acc=0.832 | Val Loss=1.5215, Acc=0.640 | LR=1.29e-04\n",
      "[Epoch 039] Train Loss=0.9976, Acc=0.829 | Val Loss=1.5216, Acc=0.641 | LR=1.66e-04\n",
      "[Epoch 040] Train Loss=0.9953, Acc=0.827 | Val Loss=1.5286, Acc=0.635 | LR=2.07e-04\n",
      "\n",
      "Early stopping at epoch 40\n",
      "VIDEO-ONLY Val Acc: 0.644\n",
      "\n",
      "[3/3] Training FUSION model...\n",
      "Model parameters: 992,528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/ice1/0/8/mdoutre3/py311venv/lib/python3.11/site-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 001] Train Loss=2.0547, Acc=0.402 | Val Loss=1.8137, Acc=0.482 | LR=1.00e-03\n",
      "  → Saved best model (val_loss=1.8137)\n",
      "[Epoch 002] Train Loss=1.6391, Acc=0.554 | Val Loss=1.6118, Acc=0.570 | LR=9.97e-04\n",
      "  → Saved best model (val_loss=1.6118)\n",
      "[Epoch 003] Train Loss=1.4738, Acc=0.622 | Val Loss=1.5136, Acc=0.637 | LR=9.89e-04\n",
      "  → Saved best model (val_loss=1.5136)\n",
      "[Epoch 004] Train Loss=1.4061, Acc=0.653 | Val Loss=1.4444, Acc=0.637 | LR=9.76e-04\n",
      "  → Saved best model (val_loss=1.4444)\n",
      "[Epoch 005] Train Loss=1.3441, Acc=0.680 | Val Loss=1.4441, Acc=0.631 | LR=9.57e-04\n",
      "  → Saved best model (val_loss=1.4441)\n",
      "[Epoch 006] Train Loss=1.2721, Acc=0.709 | Val Loss=1.4113, Acc=0.667 | LR=9.33e-04\n",
      "  → Saved best model (val_loss=1.4113)\n",
      "[Epoch 007] Train Loss=1.2123, Acc=0.740 | Val Loss=1.3313, Acc=0.692 | LR=9.05e-04\n",
      "  → Saved best model (val_loss=1.3313)\n",
      "[Epoch 008] Train Loss=1.1796, Acc=0.755 | Val Loss=1.3545, Acc=0.688 | LR=8.72e-04\n",
      "[Epoch 009] Train Loss=1.1272, Acc=0.775 | Val Loss=1.3360, Acc=0.694 | LR=8.35e-04\n",
      "[Epoch 010] Train Loss=1.0881, Acc=0.792 | Val Loss=1.3156, Acc=0.727 | LR=7.94e-04\n",
      "  → Saved best model (val_loss=1.3156)\n",
      "[Epoch 011] Train Loss=1.0584, Acc=0.808 | Val Loss=1.3633, Acc=0.704 | LR=7.50e-04\n",
      "[Epoch 012] Train Loss=1.0311, Acc=0.816 | Val Loss=1.2908, Acc=0.724 | LR=7.04e-04\n",
      "  → Saved best model (val_loss=1.2908)\n",
      "[Epoch 013] Train Loss=0.9820, Acc=0.838 | Val Loss=1.2857, Acc=0.719 | LR=6.55e-04\n",
      "  → Saved best model (val_loss=1.2857)\n",
      "[Epoch 014] Train Loss=0.9630, Acc=0.846 | Val Loss=1.3317, Acc=0.694 | LR=6.04e-04\n",
      "[Epoch 015] Train Loss=0.9326, Acc=0.860 | Val Loss=1.3095, Acc=0.711 | LR=5.53e-04\n",
      "[Epoch 016] Train Loss=0.8928, Acc=0.880 | Val Loss=1.3779, Acc=0.704 | LR=5.01e-04\n",
      "[Epoch 017] Train Loss=0.8638, Acc=0.894 | Val Loss=1.3592, Acc=0.723 | LR=4.48e-04\n",
      "[Epoch 018] Train Loss=0.8410, Acc=0.897 | Val Loss=1.3791, Acc=0.710 | LR=3.97e-04\n",
      "[Epoch 019] Train Loss=0.8019, Acc=0.924 | Val Loss=1.3811, Acc=0.715 | LR=3.46e-04\n",
      "[Epoch 020] Train Loss=0.7766, Acc=0.931 | Val Loss=1.3662, Acc=0.729 | LR=2.97e-04\n",
      "[Epoch 021] Train Loss=0.7494, Acc=0.948 | Val Loss=1.3611, Acc=0.737 | LR=2.51e-04\n",
      "[Epoch 022] Train Loss=0.7368, Acc=0.953 | Val Loss=1.3851, Acc=0.719 | LR=2.07e-04\n",
      "[Epoch 023] Train Loss=0.7136, Acc=0.962 | Val Loss=1.3884, Acc=0.729 | LR=1.66e-04\n",
      "[Epoch 024] Train Loss=0.6994, Acc=0.969 | Val Loss=1.3712, Acc=0.730 | LR=1.29e-04\n",
      "[Epoch 025] Train Loss=0.6778, Acc=0.980 | Val Loss=1.3855, Acc=0.734 | LR=9.64e-05\n",
      "[Epoch 026] Train Loss=0.6754, Acc=0.979 | Val Loss=1.3867, Acc=0.734 | LR=6.79e-05\n",
      "[Epoch 027] Train Loss=0.6647, Acc=0.982 | Val Loss=1.4015, Acc=0.733 | LR=4.42e-05\n",
      "[Epoch 028] Train Loss=0.6607, Acc=0.986 | Val Loss=1.3961, Acc=0.731 | LR=2.54e-05\n",
      "\n",
      "Early stopping at epoch 28\n",
      "\n",
      "======================================================================\n",
      "RESULTS SUMMARY\n",
      "======================================================================\n",
      "Text-only:   0.515\n",
      "Video-only:  0.644\n",
      "Fusion:      0.719\n",
      "\n",
      "Fusion improvement over text: 0.204\n",
      "Fusion improvement over video: 0.075\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Prepare data\n",
    "    (video_paths, text_paths, labels, event_to_idx, \n",
    "     class_counts, max_seq_len) = prepare_dataset(max_seq_len=100)\n",
    "    \n",
    "    # Split indices\n",
    "    indices = list(range(len(labels)))\n",
    "    np.random.seed(42)\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    train_size = int(0.8 * len(indices))\n",
    "    train_indices = indices[:train_size]\n",
    "    val_indices = indices[train_size:]\n",
    "    \n",
    "    # Create separate datasets for train and val\n",
    "    train_dataset = SoccerFusionDataset(\n",
    "        video_paths=[video_paths[i] for i in train_indices],\n",
    "        text_paths=[text_paths[i] for i in train_indices],\n",
    "        labels=[labels[i] for i in train_indices],\n",
    "        max_seq_len=max_seq_len,\n",
    "        augment=False\n",
    "    )\n",
    "    \n",
    "    val_dataset = SoccerFusionDataset(\n",
    "        video_paths=[video_paths[i] for i in val_indices],\n",
    "        text_paths=[text_paths[i] for i in val_indices],\n",
    "        labels=[labels[i] for i in val_indices],\n",
    "        max_seq_len=max_seq_len,\n",
    "        augment=False\n",
    "    )\n",
    "    \n",
    "    train_subset = train_dataset\n",
    "    val_subset = val_dataset\n",
    "    \n",
    "    train_loader = DataLoader(train_subset, batch_size=32, shuffle=True, \n",
    "                             num_workers=4, pin_memory=True, drop_last=True)\n",
    "    val_loader = DataLoader(val_subset, batch_size=32, shuffle=False, \n",
    "                           num_workers=4, pin_memory=True)\n",
    "    \n",
    "    print(f\"\\nTrain: {len(train_subset)} samples ({len(train_loader)} batches)\")\n",
    "    print(f\"Val:   {len(val_subset)} samples ({len(val_loader)} batches)\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # BASELINE EXPERIMENTS: Test text-only vs video-only\n",
    "    # =========================================================================\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"BASELINE EXPERIMENTS\")\n",
    "    print(\"=\"*70)\n",
    "      \n",
    "    # TEXT-ONLY MODEL\n",
    "    print(\"\\n[1/3] Training TEXT-ONLY baseline...\")\n",
    "    class TextOnlyModel(nn.Module):\n",
    "        def __init__(self, input_dim=768, hidden_dim=256, num_classes=15, dropout=0.1):\n",
    "            super().__init__()\n",
    "            self.classifier = nn.Sequential(\n",
    "                nn.Linear(input_dim, hidden_dim),\n",
    "                nn.LayerNorm(hidden_dim),\n",
    "                nn.GELU(),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "                nn.LayerNorm(hidden_dim // 2),\n",
    "                nn.GELU(),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(hidden_dim // 2, num_classes)\n",
    "            )\n",
    "        \n",
    "        def forward(self, video_x, video_mask, text_emb):\n",
    "            return self.classifier(text_emb)\n",
    "    \n",
    "    text_model = TextOnlyModel(num_classes=len(event_to_idx), dropout=0.1)\n",
    "    text_model = train_model(text_model, train_loader, val_loader, epochs=50, \n",
    "                            lr=1e-3, patience=10, device=device, \n",
    "                            class_counts=class_counts)\n",
    "    text_val_loss, text_val_acc = validate(text_model, val_loader, \n",
    "                                           nn.CrossEntropyLoss(), device)\n",
    "    print(f\"TEXT-ONLY Val Acc: {text_val_acc:.3f}\")\n",
    "    \n",
    "    # VIDEO-ONLY MODEL\n",
    "    print(\"\\n[2/3] Training VIDEO-ONLY baseline...\")\n",
    "    class VideoOnlyModel(nn.Module):\n",
    "        def __init__(self, input_dim=768, d_model=256, nhead=4, num_layers=2,\n",
    "                     num_classes=15, dropout=0.1, max_seq_len=100):\n",
    "            super().__init__()\n",
    "            self.input_proj = nn.Linear(input_dim, d_model)\n",
    "            self.pos_encoder = PositionalEncoding(d_model, max_seq_len, dropout=0.1)\n",
    "            \n",
    "            encoder_layer = nn.TransformerEncoderLayer(\n",
    "                d_model=d_model, nhead=nhead, dim_feedforward=d_model * 2,\n",
    "                dropout=dropout, batch_first=True\n",
    "            )\n",
    "            self.encoder = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "            self.classifier = nn.Sequential(\n",
    "                nn.Linear(d_model, d_model // 2),\n",
    "                nn.GELU(),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(d_model // 2, num_classes)\n",
    "            )\n",
    "        \n",
    "        def forward(self, video_x, video_mask, text_emb):\n",
    "            x = self.input_proj(video_x)\n",
    "            x = self.pos_encoder(x)\n",
    "            x = self.encoder(x, src_key_padding_mask=~video_mask)\n",
    "            x = x.mean(dim=1)  # Average pooling\n",
    "            return self.classifier(x)\n",
    "    \n",
    "    video_model = VideoOnlyModel(num_classes=len(event_to_idx), dropout=0.1)\n",
    "    video_model = train_model(video_model, train_loader, val_loader, epochs=50, \n",
    "                             lr=1e-3, patience=15, device=device,\n",
    "                             class_counts=class_counts)\n",
    "    video_val_loss, video_val_acc = validate(video_model, val_loader, \n",
    "                                            nn.CrossEntropyLoss(), device)\n",
    "    print(f\"VIDEO-ONLY Val Acc: {video_val_acc:.3f}\")\n",
    "    \n",
    "    # FUSION MODEL\n",
    "    print(\"\\n[3/3] Training FUSION model...\")\n",
    "    model = MultimodalFusionTransformer(\n",
    "        input_dim=768,\n",
    "        d_model=256,\n",
    "        nhead=4,\n",
    "        num_layers=2,\n",
    "        num_classes=len(event_to_idx),\n",
    "        dropout=0.1,\n",
    "        max_seq_len=100\n",
    "    )\n",
    "    \n",
    "    num_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Model parameters: {num_params:,}\")\n",
    "    \n",
    "    model = train_model(\n",
    "        model, train_loader, val_loader,\n",
    "        epochs=30,\n",
    "        lr=1e-3,\n",
    "        patience=15,\n",
    "        device=device,\n",
    "        class_counts=class_counts\n",
    "    )\n",
    "    \n",
    "    fusion_val_loss, fusion_val_acc = validate(\n",
    "        model, val_loader, nn.CrossEntropyLoss(), device\n",
    "    )\n",
    "    \n",
    "    # SUMMARY\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"RESULTS SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Text-only:   {text_val_acc:.3f}\")\n",
    "    print(f\"Video-only:  {video_val_acc:.3f}\")\n",
    "    print(f\"Fusion:      {fusion_val_acc:.3f}\")\n",
    "    print(f\"\\nFusion improvement over text: {(fusion_val_acc - text_val_acc):.3f}\")\n",
    "    print(f\"Fusion improvement over video: {(fusion_val_acc - video_val_acc):.3f}\")\n",
    "    print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7c8af5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "667d9a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def per_class_accuracy(model, loader, device, num_classes):\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for video_x, video_mask, text_emb, labels in loader:\n",
    "            video_x = video_x.to(device)\n",
    "            video_mask = video_mask.to(device)\n",
    "            text_emb = text_emb.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            logits = model(video_x, video_mask, text_emb)\n",
    "            preds = logits.argmax(dim=1)\n",
    "\n",
    "            all_preds.append(preds.cpu().numpy())\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "\n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds, labels=list(range(num_classes)))\n",
    "    \n",
    "    # Per-class accuracy\n",
    "    per_class = cm.diagonal() / cm.sum(axis=1).clip(min=1)\n",
    "    \n",
    "    return per_class, cm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6ef7f6f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing per-class performance...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/ice1/0/8/mdoutre3/py311venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "/storage/ice1/0/8/mdoutre3/py311venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "/storage/ice1/0/8/mdoutre3/py311venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class                     | Text  | Video | Fusion\n",
      "-------------------------------------------------------\n",
      "Ball out of play          | 0.809 | 0.834 | 0.868\n",
      "Clearance                 | 0.247 | 0.588 | 0.600\n",
      "Corner                    | 0.312 | 0.729 | 0.833\n",
      "Direct free-kick          | 0.143 | 0.429 | 0.381\n",
      "Foul                      | 0.504 | 0.568 | 0.799\n",
      "Goal                      | 0.333 | 0.500 | 0.292\n",
      "Indirect free-kick        | 0.260 | 0.375 | 0.583\n",
      "Kick-off                  | 0.125 | 0.469 | 0.344\n",
      "Offside                   | 0.393 | 0.000 | 0.321\n",
      "Penalty                   | 0.000 | 0.000 | 0.000\n",
      "Red card                  | 0.000 | 0.000 | 0.000\n",
      "Shots off target          | 0.471 | 0.412 | 0.686\n",
      "Shots on target           | 0.311 | 0.426 | 0.279\n",
      "Substitution              | 0.643 | 0.643 | 0.857\n",
      "Throw-in                  | 0.512 | 0.808 | 0.837\n",
      "Yellow card               | 0.667 | 0.667 | 0.762\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nComputing per-class performance...\\n\")\n",
    "\n",
    "num_classes = len(event_to_idx)\n",
    "idx_to_event = {v: k for k, v in event_to_idx.items()}\n",
    "\n",
    "# TEXT-ONLY\n",
    "text_pc, text_cm = per_class_accuracy(text_model, val_loader, device, num_classes)\n",
    "\n",
    "# VIDEO-ONLY\n",
    "video_pc, video_cm = per_class_accuracy(video_model, val_loader, device, num_classes)\n",
    "\n",
    "# FUSION\n",
    "fusion_pc, fusion_cm = per_class_accuracy(model, val_loader, device, num_classes)\n",
    "\n",
    "# Pretty print\n",
    "print(f\"{'Class':25s} | Text  | Video | Fusion\")\n",
    "print(\"-\"*55)\n",
    "for i in range(num_classes):\n",
    "    cls = idx_to_event[i][:23]  # shorten\n",
    "    print(f\"{cls:25s} | {text_pc[i]:.3f} | {video_pc[i]:.3f} | {fusion_pc[i]:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b87ff1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (GPU311)",
   "language": "python",
   "name": "gpu311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
